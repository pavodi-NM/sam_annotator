{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SAM Annotator Documentation","text":"<p>Welcome to the documentation for SAM Annotator, a general-purpose image annotation tool based on the Segment Anything Model (SAM).</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>Getting Started</li> <li>User Guide</li> <li>Advanced Features</li> <li>API Reference</li> <li>Development</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>SAM Annotator is a powerful tool that allows you to annotate images using the Segment Anything Model. To get started:</p> <pre><code># Install SAM Annotator\npip install sam-annotator\n\n# Launch the annotator\nsam_annotator /path/to/images\n</code></pre>"},{"location":"#user-guide","title":"User Guide","text":"<ul> <li>Keyboard Shortcuts: Complete guide to keyboard shortcuts for efficient annotation</li> <li>Loading and Saving Annotations (coming soon)</li> <li>Annotation Formats (coming soon)</li> <li>Configuration Options (coming soon)</li> </ul>"},{"location":"#advanced-features","title":"Advanced Features","text":"<ul> <li>Customizing the Annotation Process (coming soon)</li> <li>Working with Large Datasets (coming soon)</li> <li>Integration with Other Tools (coming soon)</li> </ul>"},{"location":"#api-reference","title":"API Reference","text":"<p>SAM Annotator can also be used programmatically through its Python API:</p> <ul> <li>Core Components (coming soon)</li> <li>Annotations Management (coming soon)</li> <li>Model Integration (coming soon)</li> </ul>"},{"location":"#development","title":"Development","text":"<ul> <li>Contributing Guidelines (coming soon)</li> <li>Building from Source (coming soon)</li> <li>Architecture Overview (coming soon)</li> </ul>"},{"location":"#repository","title":"Repository","text":"<p>The source code is available on GitHub: pavodi-nm/sam_annotator</p>"},{"location":"#license","title":"License","text":"<p>SAM Annotator is available under the MIT License. See the LICENSE file for more details. </p>"},{"location":"implementation/","title":"Implementation Details","text":"<p>This document provides an in-depth explanation of how the point-based and box-based annotation features are implemented in the SAM Annotator tool.</p>"},{"location":"implementation/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Components</li> <li>Box-Based Annotation</li> <li>Point-Based Annotation</li> <li>Annotation Data Structure</li> <li>Mask Processing</li> <li>Saving and Loading Annotations</li> <li>Visualization</li> </ol>"},{"location":"implementation/#overview","title":"Overview","text":"<p>The SAM Annotator is built around the Segment Anything Model (SAM), developed by Meta AI. SAM is designed to generate segmentation masks from various prompts including points and bounding boxes. Our application provides an interface to interact with SAM for efficient image annotation.</p> <p>Two primary annotation methods are implemented: 1. Box-based annotation: Drawing a bounding box around an object to generate a segmentation mask 2. Point-based annotation: Placing foreground and background points to guide the segmentation</p>"},{"location":"implementation/#components","title":"Components","text":"<p>The annotation system is composed of several interacting components:</p> <ul> <li>SAMAnnotator: Main orchestrator class that coordinates the entire annotation workflow</li> <li>EventHandler: Manages user interactions with the interface</li> <li>WindowManager: Handles window operations and visualization</li> <li>Predictor: Interfaces with the SAM model to generate masks</li> <li>FileManager: Manages loading and saving of annotations</li> <li>CommandManager: Implements undo/redo functionality for annotation operations</li> </ul>"},{"location":"implementation/#box-based-annotation","title":"Box-Based Annotation","text":""},{"location":"implementation/#implementation-flow","title":"Implementation Flow","text":"<ol> <li>User Interaction:</li> <li>The user draws a box by clicking and dragging</li> <li><code>EventHandler.handle_mouse_event()</code> captures the mouse events</li> <li> <p>Box coordinates are stored in <code>box_start</code> and <code>box_end</code> variables</p> </li> <li> <p>Mask Prediction:</p> </li> <li>Upon mouse release, <code>_handle_mask_prediction()</code> is called</li> <li>The method scales the box coordinates from display size to original image size</li> <li>A center point is calculated from the box for additional reference</li> <li> <p>The predictor is called with both the box and center point:      <pre><code>masks, scores, _ = self.predictor.predict(\n    point_coords=input_points,\n    point_labels=input_labels,\n    box=input_box,\n    multimask_output=True\n)\n</code></pre></p> </li> <li> <p>Processing Results:</p> </li> <li>The best mask is selected based on confidence scores</li> <li>The mask is resized to match display dimensions</li> <li>The mask is set in WindowManager: <code>self.window_manager.set_mask(display_mask)</code></li> <li>The interface is updated to show the predicted mask</li> </ol>"},{"location":"implementation/#key-functions","title":"Key Functions:","text":"<ul> <li><code>_handle_mask_prediction()</code>: Processes the box input and generates a mask</li> <li><code>EventHandler.handle_mouse_event()</code>: Captures mouse interactions for drawing the box</li> <li><code>EventHandler.reset_state()</code>: Clears the current selection state</li> </ul>"},{"location":"implementation/#point-based-annotation","title":"Point-Based Annotation","text":""},{"location":"implementation/#implementation-flow_1","title":"Implementation Flow","text":"<ol> <li>User Interaction:</li> <li>The mode is switched to 'point' (using 'p' key)</li> <li>The user clicks to place foreground points (left click) or background points (right click)</li> <li><code>EventHandler.handle_mouse_event()</code> captures these points and their labels</li> <li> <p>Points are stored in the <code>points</code> list and labels in the <code>point_labels</code> list</p> </li> <li> <p>Mask Prediction:</p> </li> <li>After points are placed, pressing 'space' triggers <code>_handle_point_prediction()</code></li> <li>The method scales the point coordinates from display size to original image size</li> <li> <p>The predictor is called with the points and their labels:      <pre><code>masks, scores, _ = self.predictor.predict(\n    point_coords=input_points,\n    point_labels=input_labels,\n    multimask_output=True\n)\n</code></pre></p> </li> <li> <p>Processing Results:</p> </li> <li>The best mask is selected based on confidence scores</li> <li>The mask is resized to match display dimensions</li> <li>The mask is set in WindowManager: <code>self.window_manager.set_mask(display_mask)</code></li> <li>The interface is updated to show the predicted mask with the input points</li> </ol>"},{"location":"implementation/#key-functions_1","title":"Key Functions:","text":"<ul> <li><code>_handle_point_prediction()</code>: Processes the point inputs and generates a mask</li> <li><code>EventHandler.handle_mouse_event()</code>: Captures mouse interactions for placing points</li> <li><code>VisualizationManager.draw_input_points()</code>: Draws the points with appropriate colors (green for foreground, red for background)</li> </ul>"},{"location":"implementation/#annotation-data-structure","title":"Annotation Data Structure","text":"<p>When an annotation is added using 'a' key, it is converted to a structured format:</p> <pre><code>annotation = {\n    'id': len(self.annotations),\n    'class_id': self.current_class_id,\n    'class_name': self.class_names[self.current_class_id],\n    'box': original_box,            # Box in original image coordinates\n    'display_box': display_box,     # Box in display coordinates\n    'contour_points': contour_points,  # OpenCV contour format\n    'contour': contour_list,        # Flattened points for visualization\n    'mask': clean_mask,             # Boolean mask\n    'area': cv2.contourArea(display_contour),\n    'metadata': {\n        'annotation_mode': self.event_handler.mode,\n        'timestamp': time.time()\n    }\n}\n</code></pre>"},{"location":"implementation/#mask-processing","title":"Mask Processing","text":"<p>After a mask is predicted, <code>_add_annotation()</code> handles the following steps:</p> <ol> <li>Contour Extraction:</li> <li>The boolean mask is converted to uint8</li> <li>Contours are extracted using <code>cv2.findContours()</code></li> <li> <p>The largest contour is selected</p> </li> <li> <p>Bounding Box Calculation:</p> </li> <li>A bounding box is calculated from the contour using <code>cv2.boundingRect()</code></li> <li> <p>The box is scaled for both display and original image dimensions</p> </li> <li> <p>Mask Cleaning:</p> </li> <li>A clean boolean mask is created</li> <li>The contour is processed into two formats:<ul> <li><code>contour_points</code>: Original cv2 contour format</li> <li><code>contour</code>: Flattened list for visualization</li> </ul> </li> </ol>"},{"location":"implementation/#saving-and-loading-annotations","title":"Saving and Loading Annotations","text":""},{"location":"implementation/#saving-process","title":"Saving Process","text":"<p>The <code>_save_annotations()</code> method handles saving annotations to disk:</p> <ol> <li>Annotations are validated to ensure they have required fields</li> <li>Original image dimensions are obtained</li> <li>The FileManager's <code>save_annotations()</code> method is called with:</li> <li>The annotations list</li> <li>Image name</li> <li>Original and display dimensions</li> <li>Class names</li> </ol> <p>The FileManager then: 1. Scales contour points back to original image space 2. Writes normalized coordinates to a text file 3. Creates visualization images of the masks 4. Saves metadata about the annotations</p>"},{"location":"implementation/#loading-process","title":"Loading Process","text":"<p>When loading an image with existing annotations via <code>_load_image()</code>:</p> <ol> <li>The image is loaded and processed to display dimensions</li> <li>The FileManager's <code>load_annotations()</code> method is called to fetch existing annotations</li> <li>Annotations are scaled to match the display dimensions</li> <li>The interface is updated to show the annotations</li> </ol>"},{"location":"implementation/#visualization","title":"Visualization","text":"<p>The <code>VisualizationManager</code> handles all rendering of annotations:</p> <ol> <li>create_composite_view(): Main method that creates a visualization with:</li> <li>Original image as background</li> <li>Colored mask overlays with adjustable opacity</li> <li>Bounding boxes</li> <li>Class labels</li> <li> <p>Interactive points (when in point mode)</p> </li> <li> <p>Drawing Functions:</p> </li> <li><code>_draw_mask()</code>: Renders a mask with the class color and proper opacity</li> <li><code>_draw_box()</code>: Draws a bounding box with the class color</li> <li><code>_draw_label()</code>: Adds a class label with a semi-transparent background</li> <li><code>draw_input_points()</code>: Visualizes input points with numbers and colors indicating foreground/background</li> </ol>"},{"location":"implementation/#command-pattern-implementation","title":"Command Pattern Implementation","text":"<p>Annotation operations use a command pattern for undo/redo functionality:</p> <ol> <li>Add Annotation: <code>AddAnnotationCommand</code> adds a new annotation to the list</li> <li>Delete Annotation: <code>DeleteAnnotationCommand</code> removes an annotation</li> <li>Modify Annotation: <code>ModifyAnnotationCommand</code> changes properties of an annotation</li> </ol> <p>Each command handles both the execution and its reverse operation, allowing for robust undo/redo capabilities. </p>"},{"location":"placeholder/","title":"Coming Soon","text":"<p>This documentation section is currently under development. Please check back later.</p> <p>Return to Documentation Home </p>"},{"location":"shortcuts/","title":"SAM Annotator Keyboard Shortcuts","text":"<p>This document provides a comprehensive guide to the keyboard shortcuts available in the SAM Annotator tool. These shortcuts help users navigate the interface and perform operations efficiently.</p>"},{"location":"shortcuts/#basic-navigation","title":"Basic Navigation","text":"Action Shortcut Description Quit Q Exit the application Next Image N Navigate to the next image in the dataset Previous Image P Navigate to the previous image in the dataset Save S Save current annotations Clear Selection X Clear the current selection Add Annotation A Add the current selection as an annotation Undo Z Undo the last action Redo Y Redo the previously undone action Clear All C Clear all annotations on the current image"},{"location":"shortcuts/#view-controls","title":"View Controls","text":"Action Shortcut Description Toggle Masks M Show/hide segmentation masks Toggle Boxes B Show/hide bounding boxes Toggle Labels L Show/hide annotation labels Toggle Points T Show/hide prompt points Toggle View Controls V Show/hide view control panel Toggle Review Mode R Enter/exit annotation review mode"},{"location":"shortcuts/#export-operations","title":"Export Operations","text":"<p>SAM Annotator supports exporting annotations to various formats using a two-key sequence:</p> <ol> <li>Press E to enter Export Mode</li> <li>Press the format key:</li> <li>C for COCO format</li> <li>Y for YOLO format</li> <li>P for Pascal VOC format</li> </ol>"},{"location":"shortcuts/#zoom-controls","title":"Zoom Controls","text":"Action Shortcut Description Zoom In = Increase zoom level Zoom Out - Decrease zoom level Reset Zoom 0 Reset zoom to 100%"},{"location":"shortcuts/#annotation-opacity-controls","title":"Annotation Opacity Controls","text":"Action Shortcut Description Increase Opacity ] Make annotations more opaque Decrease Opacity [ Make annotations more transparent"},{"location":"shortcuts/#function-keys","title":"Function Keys","text":"Action Shortcut Description Help F1 Show help information Save View Settings F2 Save current view configuration Load View Settings F3 Load saved view configuration"},{"location":"shortcuts/#using-shortcuts-effectively","title":"Using Shortcuts Effectively","text":""},{"location":"shortcuts/#tips-for-efficient-annotation","title":"Tips for Efficient Annotation","text":"<ol> <li> <p>Learn the basics first: Focus on mastering A (add annotation), X (clear selection), and S (save) for the most common workflow.</p> </li> <li> <p>Navigation efficiency: Use N and P to quickly move through images without using the mouse.</p> </li> <li> <p>Customize view: Toggle various view elements (M, B, L) based on your current task to reduce visual clutter.</p> </li> <li> <p>Review workflow: Press R to enter review mode when you need to check your annotations without making accidental changes.</p> </li> </ol>"},{"location":"shortcuts/#example-workflows","title":"Example Workflows","text":""},{"location":"shortcuts/#basic-annotation-workflow","title":"Basic Annotation Workflow","text":"<ol> <li>Navigate to an image (N/P)</li> <li>Create a selection (using mouse)</li> <li>Add the annotation (A)</li> <li>Repeat steps 2-3 for all objects</li> <li>Save work (S)</li> <li>Move to next image (N)</li> </ol>"},{"location":"shortcuts/#review-and-correction-workflow","title":"Review and Correction Workflow","text":"<ol> <li>Enter review mode (R)</li> <li>Navigate through images (N/P)</li> <li>Exit review mode to make corrections (R again)</li> <li>Use undo/redo (Z/Y) to fix errors</li> <li>Save changes (S)</li> </ol>"},{"location":"shortcuts/#customizing-shortcuts","title":"Customizing Shortcuts","text":"<p>Shortcuts can be customized by modifying the <code>shortcuts.py</code> file in the configuration directory. The default location is:</p> <pre><code>sam_annotator/config/shortcuts.py\n</code></pre> <p>To customize shortcuts, edit the <code>SHORTCUTS</code> and <code>FUNCTION_SHORTCUTS</code> dictionaries in this file.</p> <p>Example of customization: <pre><code># Changing 'next_image' from 'n' to 'right arrow'\nSHORTCUTS = {\n    # ... other shortcuts ...\n    'next_image': 'Right',  # Changed from 'n'\n    # ... other shortcuts ...\n}\n</code></pre></p> <p>Note: After customizing shortcuts, restart the application for changes to take effect. </p>"}]}