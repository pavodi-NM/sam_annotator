{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SAM Annotator Documentation","text":"<p>Welcome to the documentation for SAM Annotator, a general-purpose image annotation tool based on the Segment Anything Model (SAM).</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>Getting Started</li> <li>User Guide</li> <li>Advanced Features</li> <li>API Reference</li> <li>Development</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>SAM Annotator is a powerful tool that allows you to annotate images using the Segment Anything Model. To get started:</p> <pre><code># Install SAM Annotator\npip install sam-annotator\n\n# Launch the annotator\nsam_annotator /path/to/images\n</code></pre>"},{"location":"#user-guide","title":"User Guide","text":"<ul> <li>Keyboard Shortcuts: Complete guide to keyboard shortcuts for efficient annotation</li> <li>Loading and Saving Annotations: How annotations are stored and loaded</li> <li>Annotation Formats: Details on supported export formats</li> <li>Configuration Options: Command-line arguments and configuration settings</li> <li>Implementation Details: Technical details about how SAM Annotator works</li> </ul>"},{"location":"#advanced-features","title":"Advanced Features","text":"<ul> <li>Customizing the Annotation Process (coming soon)</li> <li>Working with Large Datasets (coming soon)</li> <li>Integration with Other Tools (coming soon)</li> </ul>"},{"location":"#api-reference","title":"API Reference","text":"<p>SAM Annotator can also be used programmatically through its Python API:</p> <ul> <li>API Reference Guide: Comprehensive documentation of the SAM Annotator API</li> <li>Core Components (coming soon)</li> <li>Annotations Management (coming soon)</li> <li>Model Integration (coming soon)</li> </ul>"},{"location":"#development","title":"Development","text":"<ul> <li>Contributing Guidelines (coming soon)</li> <li>Building from Source (coming soon)</li> <li>Architecture Overview (coming soon)</li> </ul>"},{"location":"#repository","title":"Repository","text":"<p>The source code is available on GitHub: pavodi-nm/sam_annotator</p>"},{"location":"#license","title":"License","text":"<p>SAM Annotator is available under the MIT License. See the LICENSE file for more details. </p>"},{"location":"annotation_formats/","title":"Annotation Formats","text":"<p>SAM Annotator supports multiple annotation formats for exporting your segmentation data. This guide explains the different formats, their structure, and how to use them.</p>"},{"location":"annotation_formats/#supported-export-formats","title":"Supported Export Formats","text":"<p>SAM Annotator currently supports exporting annotations to the following formats:</p> <ol> <li>COCO: The Common Objects in Context format</li> <li>YOLO: You Only Look Once format</li> <li>Pascal VOC: Pascal Visual Object Classes format</li> </ol>"},{"location":"annotation_formats/#exporting-annotations","title":"Exporting Annotations","text":"<p>You can export your annotations using the following keyboard shortcuts:</p> Format Shortcut COCO <code>E</code> YOLO <code>Y</code> Pascal VOC <code>V</code> <p>All exports are saved to the <code>exports/</code> directory within your category path, with a subdirectory for each format.</p>"},{"location":"annotation_formats/#format-details","title":"Format Details","text":""},{"location":"annotation_formats/#coco-format","title":"COCO Format","text":"<p>COCO is a large-scale object detection, segmentation, and captioning dataset format. SAM Annotator exports annotations in the COCO format as a JSON file with the following structure:</p> <pre><code>{\n\"info\": { ... },\n\"images\": [\n{\n\"id\": 1,\n\"file_name\": \"image1.jpg\",\n\"width\": 640,\n\"height\": 480\n},\n...\n],\n\"annotations\": [\n{\n\"id\": 1,\n\"image_id\": 1,\n\"category_id\": 1,\n\"segmentation\": [[x1, y1, x2, y2, ...]],\n\"area\": 1000,\n\"bbox\": [x, y, width, height],\n\"iscrowd\": 0\n},\n...\n],\n\"categories\": [\n{\n\"id\": 1,\n\"name\": \"person\",\n\"supercategory\": \"none\"\n},\n...\n]\n}\n</code></pre>"},{"location":"annotation_formats/#use-cases","title":"Use Cases:","text":"<ul> <li>Compatible with COCO API and many deep learning frameworks</li> <li>Ideal for object detection and instance segmentation tasks</li> <li>Good for large datasets with multiple classes</li> </ul>"},{"location":"annotation_formats/#yolo-format","title":"YOLO Format","text":"<p>YOLO is a popular real-time object detection system. SAM Annotator exports segmentation annotations in the YOLO segmentation format:</p> <pre><code># One file per image\n# Each line: class_id x1 y1 x2 y2 ... xn yn\n# Where coordinates are normalized (0-1)\n0 0.507 0.425 0.531 0.421 0.529 0.437 0.506 0.441\n1 0.322 0.661 0.359 0.654 0.356 0.674 0.320 0.681\n...\n</code></pre> <p>This format is compatible with YOLOv5, YOLOv8 and other versions that support segmentation.</p>"},{"location":"annotation_formats/#use-cases_1","title":"Use Cases:","text":"<ul> <li>Real-time object detection and segmentation</li> <li>Deployment on edge devices</li> <li>Training custom YOLO models</li> </ul>"},{"location":"annotation_formats/#pascal-voc-format","title":"Pascal VOC Format","text":"<p>Pascal VOC is a dataset for object class recognition. SAM Annotator exports annotations in Pascal VOC XML format:</p> <pre><code>&lt;annotation&gt;\n&lt;folder&gt;images&lt;/folder&gt;\n&lt;filename&gt;image1.jpg&lt;/filename&gt;\n&lt;size&gt;\n&lt;width&gt;640&lt;/width&gt;\n&lt;height&gt;480&lt;/height&gt;\n&lt;depth&gt;3&lt;/depth&gt;\n&lt;/size&gt;\n&lt;object&gt;\n&lt;name&gt;person&lt;/name&gt;\n&lt;bndbox&gt;\n&lt;xmin&gt;100&lt;/xmin&gt;\n&lt;ymin&gt;150&lt;/ymin&gt;\n&lt;xmax&gt;200&lt;/xmax&gt;\n&lt;ymax&gt;250&lt;/ymax&gt;\n&lt;/bndbox&gt;\n&lt;polygon&gt;\n&lt;point&gt;\n&lt;x&gt;120&lt;/x&gt;\n&lt;y&gt;160&lt;/y&gt;\n&lt;/point&gt;\n&lt;point&gt;\n&lt;x&gt;180&lt;/x&gt;\n&lt;y&gt;160&lt;/y&gt;\n&lt;/point&gt;\n...\n    &lt;/polygon&gt;\n&lt;/object&gt;\n...\n&lt;/annotation&gt;\n</code></pre>"},{"location":"annotation_formats/#use-cases_2","title":"Use Cases:","text":"<ul> <li>Compatible with older computer vision tools</li> <li>Standard format for many object detection datasets</li> <li>Easy to parse and inspect manually</li> </ul>"},{"location":"annotation_formats/#choosing-the-right-format","title":"Choosing the Right Format","text":"<p>The best format to export your annotations depends on your specific needs:</p> <ul> <li>COCO: Choose for machine learning frameworks that support COCO format, such as TensorFlow Object Detection API, Detectron2, or MMDetection</li> <li>YOLO: Best for YOLO-based models, particularly if you're using Ultralytics' YOLO implementations</li> <li>Pascal VOC: Good for compatibility with older systems or when XML-based formats are required</li> </ul>"},{"location":"annotation_formats/#format-conversion-coming-soon","title":"Format Conversion (Coming Soon)","text":"<p>Support for converting between different annotation formats directly within SAM Annotator is planned for a future release.</p>"},{"location":"annotation_formats/#external-tools","title":"External Tools","text":"<p>For more advanced conversions or formats not yet supported by SAM Annotator, consider these external tools:</p> <ol> <li>Roboflow - Supports multiple format conversions and dataset preprocessing</li> <li>LabelMe - Another annotation tool with format conversion utilities</li> <li>COCO-to-YOLO - Scripts for converting COCO JSON to YOLO format</li> </ol>"},{"location":"annotation_formats/#best-practices","title":"Best Practices","text":"<ol> <li>Export Regularly: Export your annotations frequently to prevent data loss</li> <li>Version Control: Keep track of different versions of your exported annotations</li> <li>Metadata: Document any preprocessing or specific details about your exports</li> <li>Validation: Verify that exported annotations are correctly loaded in your target application </li> </ol>"},{"location":"api_reference/","title":"SAM Annotator API Reference","text":"<p>This document provides a comprehensive guide to the SAM Annotator API, enabling programmatic access to the annotation functionality.</p>"},{"location":"api_reference/#overview","title":"Overview","text":"<p>The SAM Annotator API allows you to: - Load and initialize the SAM model - Process images programmatically - Generate masks using box or point prompts - Manage annotations  - Export annotations to various formats</p>"},{"location":"api_reference/#installation","title":"Installation","text":"<p>To use the SAM Annotator API, install the package:</p> <pre><code>pip install sam-annotator\n</code></pre>"},{"location":"api_reference/#example-scripts","title":"Example Scripts","text":"<p>The SAM Annotator package includes example scripts to help you get started quickly:</p>"},{"location":"api_reference/#simple-example","title":"Simple Example","text":"<p><code>simple_api_example.py</code> demonstrates the core functionality in a minimal script:</p> <pre><code>import os\nimport cv2\nimport numpy as np\nfrom sam_annotator.core import SAMAnnotator\n\n# Initialize the annotator\nannotator = SAMAnnotator(\n    checkpoint_path=None,  # Will use default\n    category_path=\"work_dir\",\n    classes_csv=\"classes.csv\",\n    sam_version=\"sam1\"\n)\n\n# Load an image\nimage = cv2.imread(\"path/to/image.jpg\")\n\n# Set the image in the predictor\npredictor = annotator.predictor\npredictor.set_image(image)\n\n# Generate a mask using a box prompt\nbox = np.array([100, 100, 300, 300]).reshape(1, 4)  # [x1, y1, x2, y2]\nmasks, scores, _ = predictor.predict(\n    point_coords=None,\n    point_labels=None,\n    box=box,\n    multimask_output=True\n)\n\n# Get the best mask\nmask = masks[np.argmax(scores)]\n\n# Add the mask as an annotation\nannotation = {\n    'mask': mask,\n    'class_id': 1,\n    'box': box[0],\n    'area': np.sum(mask),\n    'metadata': {'annotation_mode': 'box'}\n}\nannotation_id = annotator.annotation_manager.add_annotation(annotation)\n\n# Set the current image path (needed for saving)\nannotator.session_manager.current_image_path = \"path/to/image.jpg\"\n\n# Save the annotation\nannotator.file_manager.save_annotations(\n    annotations=[annotation],\n    image_name=\"path/to/image.jpg\",\n    original_dimensions=image.shape[:2],\n    display_dimensions=image.shape[:2],\n    class_names=[\"class1\"],\n    save_visualization=True\n)\n</code></pre>"},{"location":"api_reference/#comprehensive-example","title":"Comprehensive Example","text":"<p><code>api_example.py</code> is a full-featured example that demonstrates: - Command-line argument handling - Multiple annotation methods (box and point prompts) - Working with multiple classes - Exporting to all supported formats - Creating visualizations</p> <p>These example scripts can be found in the <code>examples/</code> directory of the SAM Annotator repository.</p>"},{"location":"api_reference/#core-components","title":"Core Components","text":"<p>The API is organized into several core components:</p>"},{"location":"api_reference/#1-samannotator","title":"1. SAMAnnotator","text":"<p>The main class that coordinates the annotation functionality.</p> <pre><code>from sam_annotator.core import SAMAnnotator\n\n# Initialize the annotator\nannotator = SAMAnnotator(\n    checkpoint_path=\"path/to/checkpoint.pth\",\n    category_path=\"path/to/category\",\n    classes_csv=\"path/to/classes.csv\",\n    sam_version=\"sam1\",  # or \"sam2\"\n    model_type=\"vit_h\"   # depends on the SAM version\n)\n\n# Access to components\npredictor = annotator.predictor  # The SAM model predictor\nannotations = annotator.annotations  # List of annotations\nfile_manager = annotator.file_manager  # Handles file operations\nsession_manager = annotator.session_manager  # Manages the current session\ncommand_manager = annotator.command_manager  # Manages undo/redo and commands\n</code></pre>"},{"location":"api_reference/#2-predictor","title":"2. Predictor","text":"<p>The interface to the SAM model for generating masks.</p> <pre><code># Get the predictor from the annotator\npredictor = annotator.predictor\n\n# Load an image\nimage = cv2.imread(\"path/to/image.jpg\")\n\n# Important: Always set the image in the predictor before prediction\npredictor.set_image(image)\n\n# Predict with a box\nbox = np.array([100, 100, 300, 300]).reshape(1, 4)  # [x1, y1, x2, y2]\nmasks, scores, logits = predictor.predict(\n    point_coords=None,\n    point_labels=None,\n    box=box,\n    multimask_output=True\n)\n\n# Predict with points\npoint_coords = np.array([[100, 100], [200, 200]])\npoint_labels = np.array([1, 1])  # 1 for foreground, 0 for background\nmasks, scores, logits = predictor.predict(\n    point_coords=point_coords,\n    point_labels=point_labels,\n    box=None,\n    multimask_output=True\n)\n</code></pre>"},{"location":"api_reference/#3-command-manager","title":"3. Command Manager","text":"<p>Manages operations on annotations, including adding, deleting, and modifying annotations.</p> <pre><code># Import the necessary command\nfrom sam_annotator.core.command_manager import AddAnnotationCommand\n\n# Create an annotation structure\nmask = masks[np.argmax(scores)]  # Get the best mask\n\n# Need to create contours from the mask\nmask_uint8 = mask.astype(np.uint8) * 255\ncontours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_KCOS)\ncontour = max(contours, key=cv2.contourArea)  # Get largest contour\n\n# Create flattened contour list\ncontour_list = contour.tolist()\nif len(contour_list) &gt; 0 and isinstance(contour_list[0], list) and len(contour_list[0]) == 1:\n    contour_list = [point[0] for point in contour_list]\n\n# Create annotation dictionary\nannotation = {\n    'id': len(annotator.annotations),  # Next available ID\n    'class_id': 1,\n    'class_name': annotator.class_names[1],\n    'box': [100, 100, 300, 300],\n    'contour': contour_list,  # Flattened points\n    'contour_points': contour,  # Original OpenCV contour\n    'mask': mask,  # Boolean numpy array\n    'display_box': [100, 100, 300, 300],\n    'area': np.sum(mask),\n    'metadata': {'annotation_mode': 'box'}\n}\n\n# Add annotation using the command manager\ncommand = AddAnnotationCommand(annotator.annotations, annotation, annotator.window_manager)\nannotator.command_manager.execute(command)\n\n# Note: There's no direct \"annotation_manager\" - annotations are stored directly in annotator.annotations\n</code></pre>"},{"location":"api_reference/#4-sessionmanager","title":"4. SessionManager","text":"<p>Manages the current annotation session, including storing current image path and navigating between images.</p> <pre><code># Get the session manager from the annotator\nsession_manager = annotator.session_manager\n\n# Set the current image path (required for saving)\nsession_manager.current_image_path = \"path/to/image.jpg\"\n\n# Save annotations for the current image\nsession_manager.save_annotations()\n\n# Navigate to next/previous image\nnext_path = session_manager.next_image()\nprev_path = session_manager.previous_image()\n\n# Check if navigation is possible\ncan_go_next = session_manager.can_move_next()\ncan_go_prev = session_manager.can_move_prev()\n\n# Get the current image path\ncurrent_path = session_manager.get_current_image_path()\n</code></pre>"},{"location":"api_reference/#5-filemanager","title":"5. FileManager","text":"<p>Handles file operations like loading/saving annotations and exporting to different formats.</p> <pre><code># Get the file manager from the annotator\nfile_manager = annotator.file_manager\n\n# Export annotations to different formats\ncoco_path = file_manager.handle_export(\"coco\", class_names)\nyolo_path = file_manager.handle_export(\"yolo\", class_names)\npascal_path = file_manager.handle_export(\"pascal\", class_names)\n</code></pre>"},{"location":"api_reference/#basic-usage","title":"Basic Usage","text":""},{"location":"api_reference/#initializing-the-annotator","title":"Initializing the Annotator","text":"<pre><code>from sam_annotator.core import SAMAnnotator\n\n# Initialize the annotator\nannotator = SAMAnnotator(\n    checkpoint_path=\"path/to/checkpoint.pth\",\n    category_path=\"path/to/category\",\n    classes_csv=\"path/to/classes.csv\",\n    sam_version=\"sam1\",\n    model_type=\"vit_h\"\n)\n</code></pre>"},{"location":"api_reference/#loading-an-image","title":"Loading an Image","text":"<pre><code># Set the current image path in session manager\nannotator.session_manager.current_image_path = \"path/to/image.jpg\"\n\n# Or load directly for prediction\nimage = cv2.imread(\"path/to/image.jpg\")\nannotator.predictor.set_image(image)\n</code></pre>"},{"location":"api_reference/#generating-a-mask-with-a-box-prompt","title":"Generating a Mask with a Box Prompt","text":"<pre><code># Define a bounding box [x1, y1, x2, y2]\nbox = np.array([100, 100, 300, 300]).reshape(1, 4)\n\n# Set the image in the predictor\npredictor = annotator.predictor\npredictor.set_image(image)\n\n# Generate masks\nmasks, scores, _ = predictor.predict(\n    point_coords=None,\n    point_labels=None,\n    box=box,\n    multimask_output=True\n)\n\n# Get the best mask\nmask_idx = np.argmax(scores)\nmask = masks[mask_idx]\n</code></pre>"},{"location":"api_reference/#generating-a-mask-with-point-prompts","title":"Generating a Mask with Point Prompts","text":"<pre><code># Define foreground points [[x1, y1], [x2, y2], ...]\nforeground_points = np.array([[150, 150], [200, 200]])\nforeground_labels = np.array([1, 1])  # 1 for foreground\n\n# Define background points (optional)\nbackground_points = np.array([[50, 50], [350, 350]])\nbackground_labels = np.array([0, 0])  # 0 for background\n\n# Combine points and labels\npoint_coords = np.vstack((foreground_points, background_points))\npoint_labels = np.hstack((foreground_labels, background_labels))\n\n# Generate masks\nmasks, scores, _ = predictor.predict(\n    point_coords=point_coords,\n    point_labels=point_labels,\n    box=None,\n    multimask_output=True\n)\n\n# Get the best mask\nmask_idx = np.argmax(scores)\nmask = masks[mask_idx]\n</code></pre>"},{"location":"api_reference/#adding-an-annotation","title":"Adding an Annotation","text":"<pre><code>from sam_annotator.core.command_manager import AddAnnotationCommand\n\n# Create contours from the mask\nmask_uint8 = mask.astype(np.uint8) * 255\ncontours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_KCOS)\nif contours:\n    contour = max(contours, key=cv2.contourArea)\n\n    # Create flattened contour list\n    contour_list = contour.tolist()\n    if len(contour_list) &gt; 0 and isinstance(contour_list[0], list) and len(contour_list[0]) == 1:\n        contour_list = [point[0] for point in contour_list]\n\n    # Create an annotation structure\n    annotation = {\n        'id': len(annotator.annotations),\n        'class_id': 1,\n        'class_name': annotator.class_names[1],\n        'box': [100, 100, 300, 300],\n        'contour': contour_list,\n        'contour_points': contour,\n        'mask': mask,\n        'display_box': [100, 100, 300, 300],\n        'area': np.sum(mask),\n        'metadata': {'annotation_mode': 'box'}\n    }\n\n    # Add the annotation using the command manager\n    command = AddAnnotationCommand(annotator.annotations, annotation, annotator.window_manager)\n    annotator.command_manager.execute(command)\n</code></pre>"},{"location":"api_reference/#saving-annotations","title":"Saving Annotations","text":"<pre><code># Make sure the session manager knows about the current image\nannotator.session_manager.current_image_path = \"path/to/image.jpg\"\n\n# Save annotations\nsuccess = annotator.file_manager.save_annotations(\n    annotations=[annotation],\n    image_name=\"path/to/image.jpg\",\n    original_dimensions=image.shape[:2],\n    display_dimensions=image.shape[:2],\n    class_names=[\"class1\"],\n    save_visualization=True\n)\n</code></pre>"},{"location":"api_reference/#exporting-annotations","title":"Exporting Annotations","text":"<pre><code># Export all annotations to COCO format\ncoco_path = annotator.file_manager.handle_export(\"coco\", annotator.class_names)\n\n# Export to YOLO format\nyolo_path = annotator.file_manager.handle_export(\"yolo\", annotator.class_names)\n\n# Export to Pascal VOC format\npascal_path = annotator.file_manager.handle_export(\"pascal\", annotator.class_names)\n</code></pre>"},{"location":"api_reference/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api_reference/#batch-processing","title":"Batch Processing","text":"<p>Process multiple images in a folder:</p> <pre><code>import os\nimport cv2\nimport numpy as np\nfrom sam_annotator.core import SAMAnnotator\n\n# Initialize\nannotator = SAMAnnotator(\n    checkpoint_path=\"path/to/checkpoint.pth\",\n    category_path=\"path/to/category\",\n    classes_csv=\"path/to/classes.csv\"\n)\n\n# Get all images in the images folder\nimage_folder = os.path.join(annotator.category_path, \"images\")\nimage_files = [f for f in os.listdir(image_folder) \n               if f.endswith(('.jpg', '.jpeg', '.png'))]\n\n# Get the predictor\npredictor = annotator.predictor\n\nfor image_file in image_files:\n    image_path = os.path.join(image_folder, image_file)\n\n    # Load image\n    image = cv2.imread(image_path)\n    predictor.set_image(image)\n\n    # Set the current image path in the session manager\n    annotator.session_manager.current_image_path = image_path\n\n    # Example: generate a mask for center of the image\n    height, width = image.shape[:2]\n    center_x, center_y = width // 2, height // 2\n    box_size = min(width, height) // 3\n\n    box = np.array([\n        center_x - box_size // 2, \n        center_y - box_size // 2,\n        center_x + box_size // 2, \n        center_y + box_size // 2\n    ]).reshape(1, 4)\n\n    # Generate mask\n    masks, scores, _ = predictor.predict(\n        point_coords=None,\n        point_labels=None,\n        box=box,\n        multimask_output=True\n    )\n\n    # Get the best mask\n    if scores.size &gt; 0:\n        mask_idx = np.argmax(scores)\n        mask = masks[mask_idx]\n\n        # Add annotation\n        annotation = {\n            'mask': mask,\n            'class_id': 1,\n            'box': box[0].tolist(),\n            'area': np.sum(mask),\n            'metadata': {'annotation_mode': 'box'}\n        }\n\n        annotator.annotation_manager.add_annotation(annotation)\n\n    # Save annotations\n    annotator.file_manager.save_annotations(\n        annotations=[annotation],\n        image_name=image_file,\n        original_dimensions=(height, width),\n        display_dimensions=(height, width),\n        class_names=[\"class1\"],\n        save_visualization=True\n    )\n\n# Export all annotations\nannotator.file_manager.handle_export(\"coco\", annotator.class_names)\n</code></pre>"},{"location":"api_reference/#api-reference","title":"API Reference","text":""},{"location":"api_reference/#samannotator-class","title":"SAMAnnotator Class","text":"<pre><code>class SAMAnnotator:\n\"\"\"Main class coordinating SAM-based image annotation.\"\"\"\n    def __init__(self, \n                checkpoint_path: str,\n                category_path: str,\n                classes_csv: str,\n                sam_version: str = 'sam1',\n                model_type: str = None):\n\"\"\"Initialize SAM annotator with all components.\"\"\"\n\n    # Internal method, not meant for direct API use:\n    def _load_image(self, image_path: str) -&gt; None:\n\"\"\"Internal method to load image and its existing annotations.\"\"\"\n</code></pre>"},{"location":"api_reference/#predictor-classes","title":"Predictor Classes","text":"<pre><code>class BaseSAMPredictor:\n\"\"\"Base class for SAM predictors.\"\"\"\n    def initialize(self, checkpoint_path: str) -&gt; None:\n\"\"\"Initialize the predictor with a model checkpoint.\"\"\"\n\n    def predict(self, \n               point_coords: np.ndarray = None,\n               point_labels: np.ndarray = None,\n               box: np.ndarray = None,\n               multimask_output: bool = True) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n\"\"\"Generate masks from the provided prompts.\"\"\"\n</code></pre>"},{"location":"api_reference/#annotationmanager-class","title":"AnnotationManager Class","text":"<pre><code>class AnnotationManager:\n\"\"\"Manages annotations and their operations.\"\"\"\n    def add_annotation(self, annotation: Dict) -&gt; int:\n\"\"\"Add a new annotation.\"\"\"\n\n    def delete_annotation(self, annotation_id: int) -&gt; bool:\n\"\"\"Delete an annotation by ID.\"\"\"\n\n    def select_annotation(self, annotation_id: int) -&gt; Dict:\n\"\"\"Select an annotation by ID.\"\"\"\n\n    def modify_annotation(self, annotation_id: int, properties: Dict) -&gt; bool:\n\"\"\"Modify properties of an annotation.\"\"\"\n</code></pre>"},{"location":"api_reference/#filemanager-class","title":"FileManager Class","text":"<pre><code>class FileManager:\n\"\"\"Manages file operations for annotations.\"\"\"\n    def load_annotations(self, image_path: str) -&gt; List[Dict]:\n\"\"\"Load annotations for the specified image.\"\"\"\n\n    def save_annotations(self, \n                        annotations: List[Dict],\n                        image_name: str,\n                        original_dimensions: Tuple[int, int],\n                        display_dimensions: Tuple[int, int],\n                        class_names: List[str]) -&gt; bool:\n\"\"\"Save annotations for the specified image.\"\"\"\n\n    def handle_export(self, format: str, class_names: List[str]) -&gt; str:\n\"\"\"Export annotations to the specified format.\"\"\"\n</code></pre>"},{"location":"api_reference/#building-custom-extensions","title":"Building Custom Extensions","text":""},{"location":"api_reference/#creating-a-custom-predictor","title":"Creating a Custom Predictor","text":"<p>You can extend the predictor functionality by creating a custom predictor:</p> <pre><code>from sam_annotator.core import BaseSAMPredictor\nimport numpy as np\n\nclass CustomPredictor(BaseSAMPredictor):\n    def initialize(self, checkpoint_path: str) -&gt; None:\n\"\"\"Initialize with custom logic.\"\"\"\n        # Your custom initialization code\n\n    def predict(self, \n               point_coords: np.ndarray = None,\n               point_labels: np.ndarray = None,\n               box: np.ndarray = None,\n               multimask_output: bool = True) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n\"\"\"Custom prediction implementation.\"\"\"\n        # Your custom prediction logic\n\n        # Return format: (masks, scores, logits)\n        return masks, scores, logits\n</code></pre>"},{"location":"api_reference/#creating-a-custom-exporter","title":"Creating a Custom Exporter","text":"<p>You can create a custom exporter for a new annotation format:</p> <pre><code>from sam_annotator.data.exporters import BaseExporter\n\nclass CustomExporter(BaseExporter):\n    def __init__(self, base_path: str):\n        super().__init__(base_path)\n\n    def export(self) -&gt; str:\n\"\"\"Export annotations to a custom format.\"\"\"\n        # Get annotation data\n        annotations = self.load_all_annotations()\n\n        # Process annotations into custom format\n        # ...\n\n        # Save to file\n        export_path = self._get_export_path(\"custom\")\n        with open(export_path, 'w') as f:\n            # Write your custom format\n            pass\n\n        return export_path\n</code></pre>"},{"location":"api_reference/#error-handling","title":"Error Handling","text":"<p>The API includes robust error handling for various common issues:</p> <pre><code>try:\n    # Initialize the annotator\n    annotator = SAMAnnotator(\n        checkpoint_path=\"path/to/checkpoint.pth\",\n        category_path=\"path/to/category\",\n        classes_csv=\"path/to/classes.csv\"\n    )\n\n    # Try to load an image that might not exist\n    try:\n        image = annotator.load_image(\"non_existent_image.jpg\")\n    except FileNotFoundError as e:\n        print(f\"Error loading image: {e}\")\n\n    # Try to generate a mask with invalid inputs\n    try:\n        mask = annotator.predict_mask_from_box([-100, -100, 100, 100])\n    except ValueError as e:\n        print(f\"Invalid box coordinates: {e}\")\n\nexcept Exception as e:\n    print(f\"General error: {e}\")\n</code></pre>"},{"location":"api_reference/#performance-considerations","title":"Performance Considerations","text":"<p>When using the API programmatically, consider the following for optimal performance:</p> <ol> <li>Batch Processing: Process images in batches rather than one by one to amortize model loading time</li> <li>Memory Management: Clear unused objects to free memory, especially after processing large images</li> <li>GPU Utilization: SAM benefits significantly from GPU acceleration; ensure CUDA is properly configured</li> <li>Image Sizing: Consider resizing large images before processing to improve performance</li> <li>Error Handling: Implement robust error handling to avoid interruptions during batch processing</li> </ol>"},{"location":"api_reference/#coming-soon","title":"Coming Soon","text":"<p>The following API features are planned for future releases:</p> <ol> <li>Automatic Annotation: Functionality for automatic annotation suggestions</li> <li>Annotation Refinement: Methods to refine existing annotations</li> <li>Multi-Model Support: Integration with additional segmentation models</li> <li>Async Processing: Asynchronous processing for improved performance</li> <li>Web API: REST API for remote access to annotation functionality </li> </ol>"},{"location":"configuration/","title":"Configuration Options","text":"<p>SAM Annotator provides various configuration options to customize its behavior. This guide explains the available options, how to set them, and their effects on the annotation process.</p>"},{"location":"configuration/#command-line-arguments","title":"Command Line Arguments","text":"<p>When running SAM Annotator, you can provide several command line arguments to configure its behavior:</p> <pre><code>sam_annotator --category_path &lt;path&gt; --classes_csv &lt;path&gt; [--sam_version sam1|sam2] [--model_type &lt;type&gt;] [--checkpoint &lt;path&gt;]\n</code></pre>"},{"location":"configuration/#required-arguments","title":"Required Arguments","text":"Argument Description <code>--category_path</code> Path to the root directory for your annotation project <code>--classes_csv</code> Path to the CSV file containing class definitions"},{"location":"configuration/#optional-arguments","title":"Optional Arguments","text":"Argument Description Default <code>--sam_version</code> SAM version to use ('sam1' or 'sam2') 'sam1' <code>--model_type</code> Model type for the selected SAM version 'vit_h' (SAM1) or 'small_v2' (SAM2) <code>--checkpoint</code> Path to a custom SAM model checkpoint Default weights for selected model"},{"location":"configuration/#class-definition-file","title":"Class Definition File","text":"<p>The class definition file is a CSV file that defines the annotation classes available in your project. The file should have the following format:</p> <pre><code>class_id,class_name,color\n0,background,\"0,0,0\"\n1,person,\"255,0,0\"\n2,car,\"0,255,0\"\n...\n</code></pre>"},{"location":"configuration/#fields","title":"Fields","text":"<ul> <li><code>class_id</code>: Numeric ID for the class (starts at 0)</li> <li><code>class_name</code>: Display name for the class</li> <li><code>color</code>: RGB color values for the class visualization (comma-separated)</li> </ul> <p>Note</p> <p>SAM Annotator currently supports a maximum of 15 classes (including background).</p>"},{"location":"configuration/#configuration-files-coming-soon","title":"Configuration Files (Coming Soon)","text":"<p>Support for configuration files to customize additional aspects of SAM Annotator is planned for a future release. This will include:</p> <ul> <li>UI customization options</li> <li>Default visualization settings</li> <li>Performance optimization settings</li> <li>Custom keyboard shortcuts</li> </ul>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"<p>SAM Annotator supports the following environment variables for advanced configuration:</p> Variable Description Default <code>SAM_CACHE_DIR</code> Directory for caching model weights <code>~/.cache/sam_annotator</code> <code>SAM_LOG_LEVEL</code> Logging verbosity level <code>INFO</code> <code>SAM_DEVICE</code> Device to run inference on <code>cuda</code> if available, otherwise <code>cpu</code>"},{"location":"configuration/#model-configuration","title":"Model Configuration","text":""},{"location":"configuration/#sam-version-1","title":"SAM Version 1","text":"<p>When using SAM version 1 (<code>--sam_version sam1</code>), the following model types are available:</p> Model Type Description Size Memory <code>vit_h</code> ViT-Huge backbone ~2.4GB ~16GB VRAM <code>vit_l</code> ViT-Large backbone ~1.2GB ~8GB VRAM <code>vit_b</code> ViT-Base backbone ~375MB ~4GB VRAM"},{"location":"configuration/#sam-version-2","title":"SAM Version 2","text":"<p>When using SAM version 2 (<code>--sam_version sam2</code>), the following model types are available:</p> Model Type Description Size Memory <code>tiny</code> Super-lightweight model ~36MB ~2GB VRAM <code>small</code> Small model ~47MB ~3GB VRAM <code>base</code> Base model ~93MB ~5GB VRAM <code>large</code> Large model ~312MB ~8GB VRAM <code>tiny_v2</code> Enhanced tiny model ~46MB ~3GB VRAM <code>small_v2</code> Enhanced small model ~86MB ~4GB VRAM <code>base_v2</code> Enhanced base model ~166MB ~6GB VRAM <code>large_v2</code> Enhanced large model ~637MB ~10GB VRAM"},{"location":"configuration/#view-configuration","title":"View Configuration","text":"<p>SAM Annotator allows you to customize the visualization of annotations during the annotation process:</p> Setting Keyboard Shortcut Description Mask Visibility <code>M</code> Show/hide segmentation masks Box Visibility <code>B</code> Show/hide bounding boxes Label Visibility <code>L</code> Show/hide class labels Point Visibility <code>T</code> Show/hide prompt points Mask Opacity <code>[</code> / <code>]</code> Decrease/increase mask opacity"},{"location":"configuration/#performance-considerations","title":"Performance Considerations","text":"<p>When configuring SAM Annotator, consider the following performance trade-offs:</p> <ol> <li>Model Selection: Larger models (vit_h, large_v2) provide better segmentation quality but require more VRAM and are slower.</li> <li>Image Size: Larger images will consume more memory and slow down processing.</li> <li>Batch Processing: SAM Annotator processes images one at a time, so CPU/GPU utilization may not be optimal for all configurations.</li> </ol>"},{"location":"configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Start Small: Begin with a smaller model if you're unsure about your hardware capabilities.</li> <li>Test Performance: Try different configurations on a small subset of your data before starting large annotation projects.</li> <li>Consider Image Resolution: Downscaling very high-resolution images before annotation can improve performance.</li> <li>Monitor Memory Usage: Watch for out-of-memory errors, especially when using larger models.</li> <li>Save Frequently: Remember to save annotations frequently regardless of your configuration. </li> </ol>"},{"location":"implementation/","title":"Implementation Details","text":"<p>This document provides an in-depth explanation of how the point-based and box-based annotation features are implemented in the SAM Annotator tool.</p>"},{"location":"implementation/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Components</li> <li>Box-Based Annotation</li> <li>Point-Based Annotation</li> <li>Annotation Data Structure</li> <li>Mask Processing</li> <li>Saving and Loading Annotations</li> <li>Visualization</li> </ol>"},{"location":"implementation/#overview","title":"Overview","text":"<p>The SAM Annotator is built around the Segment Anything Model (SAM), developed by Meta AI. SAM is designed to generate segmentation masks from various prompts including points and bounding boxes. Our application provides an interface to interact with SAM for efficient image annotation.</p> <p>Two primary annotation methods are implemented: 1. Box-based annotation: Drawing a bounding box around an object to generate a segmentation mask 2. Point-based annotation: Placing foreground and background points to guide the segmentation</p>"},{"location":"implementation/#components","title":"Components","text":"<p>The annotation system is composed of several interacting components:</p> <ul> <li>SAMAnnotator: Main orchestrator class that coordinates the entire annotation workflow</li> <li>EventHandler: Manages user interactions with the interface</li> <li>WindowManager: Handles window operations and visualization</li> <li>Predictor: Interfaces with the SAM model to generate masks</li> <li>FileManager: Manages loading and saving of annotations</li> <li>CommandManager: Implements undo/redo functionality for annotation operations</li> </ul>"},{"location":"implementation/#box-based-annotation","title":"Box-Based Annotation","text":""},{"location":"implementation/#implementation-flow","title":"Implementation Flow","text":"<ol> <li>User Interaction:</li> <li>The user draws a box by clicking and dragging</li> <li><code>EventHandler.handle_mouse_event()</code> captures the mouse events</li> <li> <p>Box coordinates are stored in <code>box_start</code> and <code>box_end</code> variables</p> </li> <li> <p>Mask Prediction:</p> </li> <li>Upon mouse release, <code>_handle_mask_prediction()</code> is called</li> <li>The method scales the box coordinates from display size to original image size</li> <li>A center point is calculated from the box for additional reference</li> <li> <p>The predictor is called with both the box and center point:      <pre><code>masks, scores, _ = self.predictor.predict(\n    point_coords=input_points,\n    point_labels=input_labels,\n    box=input_box,\n    multimask_output=True\n)\n</code></pre></p> </li> <li> <p>Processing Results:</p> </li> <li>The best mask is selected based on confidence scores</li> <li>The mask is resized to match display dimensions</li> <li>The mask is set in WindowManager: <code>self.window_manager.set_mask(display_mask)</code></li> <li>The interface is updated to show the predicted mask</li> </ol>"},{"location":"implementation/#key-functions","title":"Key Functions:","text":"<ul> <li><code>_handle_mask_prediction()</code>: Processes the box input and generates a mask</li> <li><code>EventHandler.handle_mouse_event()</code>: Captures mouse interactions for drawing the box</li> <li><code>EventHandler.reset_state()</code>: Clears the current selection state</li> </ul>"},{"location":"implementation/#point-based-annotation","title":"Point-Based Annotation","text":""},{"location":"implementation/#implementation-flow_1","title":"Implementation Flow","text":"<ol> <li>User Interaction:</li> <li>The mode is switched to 'point' (using 'w' key)</li> <li>The user clicks to place foreground points (left click) or background points (right click)</li> <li><code>EventHandler.handle_mouse_event()</code> captures these points and their labels</li> <li> <p>Points are stored in the <code>points</code> list and labels in the <code>point_labels</code> list</p> </li> <li> <p>Mask Prediction:</p> </li> <li>After points are placed, pressing 'space' triggers <code>_handle_point_prediction()</code></li> <li>The method scales the point coordinates from display size to original image size</li> <li> <p>The predictor is called with the points and their labels:      <pre><code>masks, scores, _ = self.predictor.predict(\n    point_coords=input_points,\n    point_labels=input_labels,\n    multimask_output=True\n)\n</code></pre></p> </li> <li> <p>Processing Results:</p> </li> <li>The best mask is selected based on confidence scores</li> <li>The mask is resized to match display dimensions</li> <li>The mask is set in WindowManager: <code>self.window_manager.set_mask(display_mask)</code></li> <li>The interface is updated to show the predicted mask with the input points</li> </ol>"},{"location":"implementation/#key-functions_1","title":"Key Functions:","text":"<ul> <li><code>_handle_point_prediction()</code>: Processes the point inputs and generates a mask</li> <li><code>EventHandler.handle_mouse_event()</code>: Captures mouse interactions for placing points</li> <li><code>VisualizationManager.draw_input_points()</code>: Draws the points with appropriate colors (green for foreground, red for background)</li> </ul>"},{"location":"implementation/#annotation-data-structure","title":"Annotation Data Structure","text":"<p>When an annotation is added using 'a' key, it is converted to a structured format:</p> <pre><code>annotation = {\n    'id': len(self.annotations),\n    'class_id': self.current_class_id,\n    'class_name': self.class_names[self.current_class_id],\n    'box': original_box,            # Box in original image coordinates\n    'display_box': display_box,     # Box in display coordinates\n    'contour_points': contour_points,  # OpenCV contour format\n    'contour': contour_list,        # Flattened points for visualization\n    'mask': clean_mask,             # Boolean mask\n    'area': cv2.contourArea(display_contour),\n    'metadata': {\n        'annotation_mode': self.event_handler.mode,\n        'timestamp': time.time()\n    }\n}\n</code></pre>"},{"location":"implementation/#mask-processing","title":"Mask Processing","text":"<p>After a mask is predicted, <code>_add_annotation()</code> handles the following steps:</p> <ol> <li>Contour Extraction:</li> <li>The boolean mask is converted to uint8</li> <li>Contours are extracted using <code>cv2.findContours()</code></li> <li> <p>The largest contour is selected</p> </li> <li> <p>Bounding Box Calculation:</p> </li> <li>A bounding box is calculated from the contour using <code>cv2.boundingRect()</code></li> <li> <p>The box is scaled for both display and original image dimensions</p> </li> <li> <p>Mask Cleaning:</p> </li> <li>A clean boolean mask is created</li> <li>The contour is processed into two formats:<ul> <li><code>contour_points</code>: Original cv2 contour format</li> <li><code>contour</code>: Flattened list for visualization</li> </ul> </li> </ol>"},{"location":"implementation/#saving-and-loading-annotations","title":"Saving and Loading Annotations","text":"<p>For a user-friendly guide on how annotations are stored and loaded, see the Loading and Saving Annotations documentation.</p>"},{"location":"implementation/#saving-process","title":"Saving Process","text":"<p>The <code>_save_annotations()</code> method handles saving annotations to disk:</p> <ol> <li>Annotations are validated to ensure they have required fields</li> <li>Original image dimensions are obtained</li> <li>The FileManager's <code>save_annotations()</code> method is called with:</li> <li>The annotations list</li> <li>Image name</li> <li>Original and display dimensions</li> <li>Class names</li> </ol> <p>The FileManager then: 1. Scales contour points back to original image space 2. Writes normalized coordinates to a text file 3. Creates visualization images of the masks 4. Saves metadata about the annotations</p>"},{"location":"implementation/#loading-process","title":"Loading Process","text":"<p>When loading an image with existing annotations via <code>_load_image()</code>:</p> <ol> <li>The image is loaded and processed to display dimensions</li> <li>The FileManager's <code>load_annotations()</code> method is called to fetch existing annotations</li> <li>Annotations are scaled to match the display dimensions</li> <li>The interface is updated to show the annotations</li> </ol>"},{"location":"implementation/#visualization","title":"Visualization","text":"<p>The <code>VisualizationManager</code> handles all rendering of annotations:</p> <ol> <li>create_composite_view(): Main method that creates a visualization with:</li> <li>Original image as background</li> <li>Colored mask overlays with adjustable opacity</li> <li>Bounding boxes</li> <li>Class labels</li> <li> <p>Interactive points (when in point mode)</p> </li> <li> <p>Drawing Functions:</p> </li> <li><code>_draw_mask()</code>: Renders a mask with the class color and proper opacity</li> <li><code>_draw_box()</code>: Draws a bounding box with the class color</li> <li><code>_draw_label()</code>: Adds a class label with a semi-transparent background</li> <li><code>draw_input_points()</code>: Visualizes input points with numbers and colors indicating foreground/background</li> </ol>"},{"location":"implementation/#command-pattern-implementation","title":"Command Pattern Implementation","text":"<p>Annotation operations use a command pattern for undo/redo functionality:</p> <ol> <li>Add Annotation: <code>AddAnnotationCommand</code> adds a new annotation to the list</li> <li>Delete Annotation: <code>DeleteAnnotationCommand</code> removes an annotation</li> <li>Modify Annotation: <code>ModifyAnnotationCommand</code> changes properties of an annotation</li> </ol> <p>Each command handles both the execution and its reverse operation, allowing for robust undo/redo capabilities. </p>"},{"location":"loading_saving/","title":"Loading and Saving Annotations","text":"<p>SAM Annotator provides robust functionality for loading and saving annotations in various formats. This guide explains how to manage your annotation data effectively.</p>"},{"location":"loading_saving/#annotation-storage-structure","title":"Annotation Storage Structure","text":"<p>When you specify a category path for your annotations, SAM Annotator creates the following directory structure:</p> <pre><code>category_path/\n\u251c\u2500\u2500 images/     # Place your images here\n\u251c\u2500\u2500 labels/     # Annotation files will be saved here\n\u251c\u2500\u2500 masks/      # Visualization of masks will be saved here\n\u251c\u2500\u2500 metadata/   # Metadata about annotations\n\u2514\u2500\u2500 exports/    # Exported annotations in various formats\n</code></pre>"},{"location":"loading_saving/#file-formats","title":"File Formats","text":"<p>SAM Annotator uses a text-based format for storing annotations internally, but can export to multiple industry-standard formats. </p>"},{"location":"loading_saving/#internal-format","title":"Internal Format","text":"<p>Annotations are stored in the <code>labels/</code> directory with <code>.txt</code> files corresponding to each image. Each line represents a polygon annotation in the following format:</p> <pre><code>class_id x1 y1 x2 y2 ... xn yn\n</code></pre> <p>Where: - <code>class_id</code> is the numeric ID of the annotation class - <code>x1 y1, x2 y2, ...</code> are normalized coordinates (0-1) of the polygon vertices</p>"},{"location":"loading_saving/#visualization","title":"Visualization","text":"<p>For each annotated image, SAM Annotator creates a visualization in the <code>masks/</code> directory. This visualization shows: - The segmentation mask on the left - The original image with a semi-transparent overlay on the right</p> <p>These visualizations make it easy to review your annotations visually.</p>"},{"location":"loading_saving/#automatic-saving","title":"Automatic Saving","text":"<p>Annotations are automatically saved when: 1. You press the <code>S</code> key 2. You navigate to another image (using <code>N</code> or <code>P</code> keys) 3. You exit the application</p>"},{"location":"loading_saving/#loading-annotations","title":"Loading Annotations","text":"<p>When you open an image that has existing annotations, SAM Annotator automatically loads them. The process works as follows:</p> <ol> <li>SAM Annotator looks for a corresponding <code>.txt</code> file in the <code>labels/</code> directory</li> <li>If found, it loads the polygon coordinates and scales them to match the current display dimensions</li> <li>Annotations are displayed on the image with their assigned class colors</li> </ol>"},{"location":"loading_saving/#manual-importing-coming-soon","title":"Manual Importing (Coming Soon)","text":"<p>Support for manually importing annotations from other tools and formats is planned for a future release.</p>"},{"location":"loading_saving/#caching","title":"Caching","text":"<p>SAM Annotator implements a caching mechanism to improve performance when working with large datasets. This means:</p> <ol> <li>Loaded annotations are kept in memory for faster access</li> <li>The cache is automatically cleared when needed to manage memory usage</li> <li>You can manually clear the cache by restarting the application</li> </ol>"},{"location":"loading_saving/#common-issues","title":"Common Issues","text":""},{"location":"loading_saving/#missing-annotations","title":"Missing Annotations","text":"<p>If annotations aren't appearing for an image: - Check if the corresponding <code>.txt</code> file exists in the <code>labels/</code> directory - Verify that the filename matches the image (same name, different extension) - Ensure the annotation file has the correct format</p>"},{"location":"loading_saving/#corrupted-annotation-files","title":"Corrupted Annotation Files","text":"<p>If annotation files become corrupted: 1. Look for backup files in the <code>backups/</code> directory 2. Restore the backup by copying it to the <code>labels/</code> directory with the correct filename</p>"},{"location":"loading_saving/#best-practices","title":"Best Practices","text":"<ol> <li>Create Regular Backups: Use the backup functionality to save your progress</li> <li>Consistent Naming: Keep image filenames consistent to avoid issues with annotation matching</li> <li>Check Visualizations: Review the mask visualizations to ensure annotations are correct</li> <li>Export Regularly: Export your annotations to standard formats periodically </li> </ol>"},{"location":"memory_management/","title":"Memory Management in SAM Annotator","text":"<p>This document provides a detailed overview of how SAM Annotator manages memory across different operating systems (Linux and Windows) and the caching systems implemented to enhance performance.</p>"},{"location":"memory_management/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Memory Management Architecture</li> <li>Cross-Platform Memory Management</li> <li>Linux-Based Systems</li> <li>Windows-Based Systems</li> <li>GPU Memory Management</li> <li>Configuration Options</li> <li>Memory Manager Implementation</li> <li>Caching Systems</li> <li>Image Processing Cache</li> <li>Prediction Cache</li> <li>Memory Optimization Strategies</li> <li>Real-Time Memory Monitoring</li> <li>Troubleshooting Memory Issues</li> <li>Testing Memory Management</li> </ol>"},{"location":"memory_management/#memory-management-architecture","title":"Memory Management Architecture","text":"<p>SAM Annotator uses a layered approach to memory management:</p> <ol> <li>GPU Memory Manager: Central component that interfaces with NVIDIA SMI or PyTorch to monitor and manage GPU memory.</li> <li>Image Processor Cache: Caches processed images to avoid redundant transformations.</li> <li>Prediction Cache: Stores recent segmentation predictions to avoid redundant computations.</li> <li>Memory Optimizers: Active memory management components that clear caches when memory usage exceeds thresholds.</li> </ol>"},{"location":"memory_management/#cross-platform-memory-management","title":"Cross-Platform Memory Management","text":""},{"location":"memory_management/#linux-based-systems","title":"Linux-Based Systems","text":"<p>On Linux systems, SAM Annotator takes advantage of more advanced memory management capabilities:</p> <ul> <li>NVIDIA SMI Integration: When available, uses NVIDIA System Management Interface to get detailed GPU memory statistics.</li> <li>Enhanced Memory Monitoring: Accesses detailed memory utilization metrics including total, used, and free memory.</li> <li>Background Process Optimization: Linux's better handling of background processes allows for more aggressive caching strategies.</li> </ul> <p>Example memory information on Linux: <pre><code>Memory before prediction: Used: 2.45 GB / Total: 8.00 GB (30.6%)\n</code></pre></p>"},{"location":"memory_management/#windows-based-systems","title":"Windows-Based Systems","text":"<p>On Windows systems, SAM Annotator implements fallback mechanisms:</p> <ul> <li>PyTorch Fallbacks: When NVIDIA SMI is not available, falls back to PyTorch's memory management functions.</li> <li>Simplified Memory Reporting: Uses a more robust approach to memory reporting to handle potential missing metrics.</li> <li>Safety Mechanisms: Implements additional safety checks to prevent KeyError issues when certain memory metrics are not available.</li> <li>Robust Error Handling: The <code>safe_get_memory_info()</code> method guarantees a valid memory information dictionary, even on Windows or CPU-only systems.</li> </ul> <p>Example memory information on Windows: <pre><code>Memory before prediction: Running on CPU - memory stats not available\n</code></pre></p>"},{"location":"memory_management/#gpu-memory-management","title":"GPU Memory Management","text":""},{"location":"memory_management/#configuration-options","title":"Configuration Options","text":"<p>SAM Annotator's memory management is highly configurable through environment variables:</p> Environment Variable Default Description <code>SAM_GPU_MEMORY_FRACTION</code> 0.9 Maximum fraction of GPU memory to use (0.0-1.0) <code>SAM_MEMORY_WARNING_THRESHOLD</code> 0.8 Memory utilization threshold for warnings (0.0-1.0) <code>SAM_MEMORY_CRITICAL_THRESHOLD</code> 0.95 Memory utilization threshold for critical errors (0.0-1.0) <code>SAM_ENABLE_MEMORY_GROWTH</code> True Whether to allow dynamic memory growth or enforce the limit strictly"},{"location":"memory_management/#memory-manager-implementation","title":"Memory Manager Implementation","text":"<p>The GPU memory management is centralized in the <code>GPUMemoryManager</code> class:</p> <pre><code>class GPUMemoryManager:\n\"\"\"Enhanced GPU memory manager with fallback options.\"\"\"\n\n    def __init__(self):\n        # Load configuration from environment variables with defaults\n        self.memory_fraction = self._get_env_float('SAM_GPU_MEMORY_FRACTION', 0.9)\n        self.warning_threshold = self._get_env_float('SAM_MEMORY_WARNING_THRESHOLD', 0.8)\n        self.critical_threshold = self._get_env_float('SAM_MEMORY_CRITICAL_THRESHOLD', 0.95)\n        self.enable_memory_growth = self._get_env_bool('SAM_ENABLE_MEMORY_GROWTH', True)\n\n        # Try to initialize NVIDIA SMI with fallbacks\n        self.nvml_initialized = False\n        try:\n            import nvidia_smi\n            nvidia_smi.nvmlInit()\n            self.nvidia_smi = nvidia_smi\n            self.nvml_initialized = True\n        except ImportError:\n            # Fallback to torch memory management\n            pass\n</code></pre> <p>Key methods: - <code>get_gpu_memory_info()</code>: Retrieves current memory statistics with platform-specific handling. - <code>safe_get_memory_info()</code>: Guarantees a valid memory info dictionary, even on Windows/CPU systems. - <code>check_memory_status()</code>: Evaluates memory usage against thresholds and returns status and warnings. - <code>optimize_memory()</code>: Forces garbage collection and cache clearing when needed. - <code>should_cache()</code>: Determines if it's safe to cache based on current memory usage.</p> <p>The memory manager is initialized by both the <code>SAM1Predictor</code> and <code>SAM2Predictor</code> classes, and is used throughout the application for memory monitoring and optimization.</p>"},{"location":"memory_management/#caching-systems","title":"Caching Systems","text":"<p>SAM Annotator employs multiple caching systems to optimize performance.</p>"},{"location":"memory_management/#image-processing-cache","title":"Image Processing Cache","text":"<p>Implemented in <code>ImageProcessor</code> class, this cache reduces the computational overhead of resizing and preprocessing images:</p> <pre><code># Cache initialization in ImageProcessor.__init__\nself._processed_cache = WeakValueDictionary()  # Cache processed images\nself._metadata_cache = {}  # Cache metadata\nself.max_cache_size = 10\n</code></pre> <p>Features: - Weak References: Uses <code>WeakValueDictionary</code> to allow cached images to be garbage collected when memory is low. - Size-Limited Cache: Maintains a maximum of 10 processed images by default. - Hash-Based Keys: Uses MD5 hashes of image data to identify cached entries. - Memory Usage Monitoring: Provides <code>get_memory_usage()</code> method to estimate cache size in bytes. - Explicit Cleanup: Offers <code>clear_cache()</code> method for forced cleanup.</p> <p>Implementation details:</p> <pre><code>def process_image(self, image: np.ndarray) -&gt; Tuple[np.ndarray, Dict]:\n\"\"\"Process image for annotation display using ScalingManager with caching.\"\"\"\n    try:\n        # Generate cache key\n        image_hash = hashlib.md5(image.tobytes()).hexdigest()\n\n        # Check cache first\n        if image_hash in self._processed_cache:\n            return self._processed_cache[image_hash], self._metadata_cache[image_hash]\n\n        # Process image using existing ScalingManager\n        processed_image, metadata = self.scaling_manager.process_image(\n            image, \n            interpolation=InterpolationMethod.AREA\n        )\n\n        # Cache the results\n        self._processed_cache[image_hash] = processed_image\n        self._metadata_cache[image_hash] = metadata\n\n        # Manage cache size for metadata\n        if len(self._metadata_cache) &gt; self.max_cache_size:\n            # Remove oldest items\n            oldest_key = next(iter(self._metadata_cache))\n            del self._metadata_cache[oldest_key]\n\n        return processed_image, metadata\n</code></pre>"},{"location":"memory_management/#prediction-cache","title":"Prediction Cache","text":"<p>Implemented in both <code>SAM1Predictor</code> and <code>SAM2Predictor</code> classes, this cache stores segmentation results to avoid redundant computations:</p> <pre><code># Cache initialization in predictor classes\nself.current_image_hash = None\nself.prediction_cache = {}\nself.max_cache_size = 50\n</code></pre> <p>Features: - Memory-Aware Caching: Only caches predictions when memory usage is below warning threshold. - Current Image Preservation: Option to retain cache entries for current image while clearing others. - Composite Keys: Uses a combination of image hash and input parameters to identify cached entries. - Automatic Cleanup: Clears older entries when cache size exceeds limit.</p> <p>Implementation details:</p> <pre><code>def predict(self, point_coords=None, point_labels=None, box=None, multimask_output=True):\n\"\"\"Predict masks with memory management.\"\"\"\n    try:\n        # Check memory status before prediction\n        status_ok, message = self.memory_manager.check_memory_status()\n        if not status_ok:\n            raise RuntimeError(message)\n\n        # Generate cache key and check cache\n        cache_key = self._generate_cache_key(point_coords, point_labels, box)\n        if cache_key in self.prediction_cache:\n            return self.prediction_cache[cache_key]\n\n        # Run prediction with optimizations\n        with torch.no_grad():\n            masks, scores, logits = self.predictor.predict(\n                point_coords=point_coords,\n                point_labels=point_labels,\n                box=box,\n                multimask_output=multimask_output\n            )\n\n            # Cache results if memory allows\n            memory_info = self.memory_manager.get_gpu_memory_info()\n            if memory_info['utilization'] &lt; self.memory_manager.warning_threshold:\n                self.prediction_cache[cache_key] = (masks, scores, logits)\n\n                # Manage cache size\n                if len(self.prediction_cache) &gt; self.max_cache_size:\n                    self.clear_cache(keep_current=True)\n\n            return masks, scores, logits\n    except Exception as e:\n        # Try to recover memory\n        self.memory_manager.optimize_memory(force=True)\n        raise\n</code></pre>"},{"location":"memory_management/#memory-optimization-strategies","title":"Memory Optimization Strategies","text":"<p>SAM Annotator employs several strategies to optimize memory usage:</p> <ol> <li>Periodic Memory Checks: Regularly checks memory usage in the main application loop:</li> </ol> <pre><code># From annotator.py run() method\nif hasattr(self.image_processor, 'get_memory_usage'):\n    memory_usage = self.image_processor.get_memory_usage()\n    if memory_usage &gt; 1e9:  # More than 1GB\n        self.logger.info(\"Clearing image cache due to high memory usage\")\n        self.image_processor.clear_cache()\n\n# Check GPU memory periodically\nif hasattr(self.predictor, 'get_memory_usage'):\n    gpu_memory = self.predictor.get_memory_usage()\n    if gpu_memory &gt; 0.8:  # Over 80% GPU memory\n        self.predictor.optimize_memory()\n</code></pre> <ol> <li> <p>Threshold-Based Cache Management: Clears caches when memory usage exceeds predefined thresholds.</p> </li> <li> <p>Explicit Garbage Collection: Forces Python's garbage collector and PyTorch's CUDA cache clearing:</p> </li> </ol> <pre><code>def optimize_memory(self, force: bool = False) -&gt; None:\n\"\"\"Optimize GPU memory usage.\"\"\"\n    if self.device.type != 'cuda':\n        return\n\n    try:\n        memory_info = self.get_gpu_memory_info()\n        if force or memory_info['utilization'] &gt;= self.warning_threshold:\n            # Clear cache\n            torch.cuda.empty_cache()\n\n            # Force garbage collection\n            import gc\n            gc.collect()\n\n            self.logger.info(\"Memory optimization performed\")\n    except Exception as e:\n        self.logger.error(f\"Error optimizing memory: {e}\")\n</code></pre> <ol> <li>TF32 Precision: Enables TF32 precision on supported NVIDIA GPUs for better performance and memory efficiency:</li> </ol> <pre><code>def _setup_gpu(self) -&gt; None:\n\"\"\"Configure GPU memory management.\"\"\"\n    try:\n        if self.enable_memory_growth:\n            # Enable memory growth\n            for device in range(torch.cuda.device_count()):\n                torch.cuda.set_per_process_memory_fraction(self.memory_fraction, device)\n\n        # Enable TF32 for better performance on Ampere GPUs\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.allow_tf32 = True\n    except Exception as e:\n        self.logger.error(f\"Error setting up GPU memory management: {e}\")\n</code></pre> <ol> <li> <p>Automatic Image Resizing: Resizes large images to reduce memory footprint using the <code>ScalingManager</code>.</p> </li> <li> <p>WeakValueDictionary for Image Cache: Uses weak references to allow the garbage collector to reclaim memory when needed.</p> </li> </ol>"},{"location":"memory_management/#real-time-memory-monitoring","title":"Real-Time Memory Monitoring","text":"<p>The SAM Annotator tool implements real-time memory monitoring that runs continuously in the main application loop. This proactive approach ensures that memory usage is kept in check during extended annotation sessions.</p>"},{"location":"memory_management/#main-loop-monitoring","title":"Main Loop Monitoring","text":"<p>The main loop in both <code>SAMAnnotator</code> and derived classes continuously monitors memory usage:</p> <pre><code># Main event loop implementation from sam_annotator.py\nwhile True:\n    # Check image processor memory usage\n    if hasattr(self.image_processor, 'get_memory_usage'):\n        memory_usage = self.image_processor.get_memory_usage()\n        if memory_usage &gt; 1e9:  # More than 1GB\n            self.logger.info(\"Clearing image cache\")\n            self.image_processor.clear_cache()\n\n    # Check GPU memory\n    if hasattr(self.predictor, 'get_memory_usage'):\n        gpu_memory = self.predictor.get_memory_usage()\n        if gpu_memory &gt; 0.8:  # Over 80% GPU memory\n            self.predictor.optimize_memory()\n\n    # Rest of the event loop...\n</code></pre>"},{"location":"memory_management/#memory-monitoring-thresholds","title":"Memory Monitoring Thresholds","text":"<p>The real-time monitoring system uses two types of thresholds:</p> <ol> <li>Absolute thresholds for image cache (1GB)</li> <li>Percentage thresholds for GPU memory (80%)</li> </ol> <p>These values were determined based on extensive testing to provide the best balance between performance and stability.</p>"},{"location":"memory_management/#adaptive-response","title":"Adaptive Response","text":"<p>The monitoring system doesn't just detect high memory usage; it responds adaptively:</p> <ol> <li>For image processing cache: When memory exceeds 1GB, it clears the image cache completely</li> <li>For GPU memory: When usage exceeds 80%, it performs a targeted optimization that includes:</li> <li>Clearing unused prediction caches</li> <li>Running PyTorch's CUDA cache empty function</li> <li>Triggering Python's garbage collector</li> </ol>"},{"location":"memory_management/#automatic-recovery","title":"Automatic Recovery","text":"<p>If memory issues occur during predictions, the system attempts automatic recovery:</p> <pre><code>try:\n    # Prediction code...\nexcept Exception as e:\n    self.logger.error(f\"Error in prediction: {str(e)}\")\n    # Try to recover memory\n    self.memory_manager.optimize_memory(force=True)\n    raise\n</code></pre> <p>This real-time monitoring and recovery system ensures that SAM Annotator remains stable even during long annotation sessions with large images or complex segmentation tasks.</p>"},{"location":"memory_management/#troubleshooting-memory-issues","title":"Troubleshooting Memory Issues","text":"<p>If you encounter memory issues when using SAM Annotator:</p>"},{"location":"memory_management/#common-issues-on-windows","title":"Common Issues on Windows","text":"<ol> <li>KeyError: 'formatted': This error occurs when the GPU memory information lacks the 'formatted' key. Now fixed with the <code>safe_get_memory_info()</code> method.</li> </ol> <p>Solution: Update to the latest version with the fix.</p> <ol> <li>Out of Memory Errors: Windows systems may experience OOM errors with large images.</li> </ol> <p>Solution: Reduce the maximum image size in the configuration or use the CPU mode.</p>"},{"location":"memory_management/#common-issues-on-linux","title":"Common Issues on Linux","text":"<ol> <li>CUDA Out of Memory: Linux systems might still have CUDA OOM errors with large batches.</li> </ol> <p>Solution: Set the environment variable <code>SAM_GPU_MEMORY_FRACTION=0.7</code> to limit GPU memory usage.</p> <ol> <li>nvidia-smi Not Found: Some Linux distributions might lack proper NVIDIA driver setup.</li> </ol> <p>Solution: Install NVIDIA drivers or set <code>SAM_ENABLE_MEMORY_GROWTH=false</code> to use PyTorch's memory management.</p>"},{"location":"memory_management/#general-optimization-tips","title":"General Optimization Tips","text":"<ol> <li>Environment Variables:</li> <li><code>SAM_GPU_MEMORY_FRACTION</code>: Controls maximum GPU memory usage (default: 0.9)</li> <li><code>SAM_MEMORY_WARNING_THRESHOLD</code>: Sets memory warning level (default: 0.8)</li> <li><code>SAM_MEMORY_CRITICAL_THRESHOLD</code>: Sets memory critical level (default: 0.95)</li> <li> <p><code>SAM_ENABLE_MEMORY_GROWTH</code>: Enables/disables memory growth (default: True)</p> </li> <li> <p>Reduce Image Size: Configure smaller target image sizes to reduce memory footprint.</p> </li> <li> <p>Limit Batch Processing: Process fewer images at once to reduce memory pressure.</p> </li> </ol>"},{"location":"memory_management/#testing-memory-management","title":"Testing Memory Management","text":"<p>SAM Annotator includes a test suite for memory management that validates:</p> <ol> <li>Memory allocation limits: Tests allocation with different memory fractions.</li> <li>Memory growth behavior: Validates behavior when memory limits are reached.</li> <li>Optimization effectiveness: Measures memory recovery after optimization.</li> </ol> <p>The test code includes a memory allocation test that tries to allocate large tensors while monitoring memory usage:</p> <pre><code>def test_memory_allocation(memory_manager, logger):\n\"\"\"Test allocating and freeing memory.\"\"\"\n    try:\n        # Initial memory state\n        initial_info = memory_manager.get_gpu_memory_info()\n        logger.info(f\"Initial GPU memory state: {format_memory_info(initial_info)}\")\n\n        # Allocate some tensors\n        tensors = []\n        for i in range(5):\n            # Allocate a 1GB tensor\n            size = 256 * 1024 * 1024  # ~1GB\n            tensor = torch.zeros(size, device='cuda')\n            tensors.append(tensor)\n\n            # Check memory status\n            status_ok, message = memory_manager.check_memory_status()\n            current_info = memory_manager.get_gpu_memory_info()\n            logger.info(f\"After allocation {i+1}: {format_memory_info(current_info)}\")\n\n            # If we hit critical threshold, break\n            if not status_ok:\n                logger.warning(\"Hit critical memory threshold!\")\n                break\n\n        # Try to optimize memory\n        logger.info(\"Attempting memory optimization...\")\n        memory_manager.optimize_memory(force=True)\n\n        # Check memory after optimization\n        post_opt_info = memory_manager.get_gpu_memory_info()\n        logger.info(f\"After optimization: {format_memory_info(post_opt_info)}\")\n\n    except Exception as e:\n        logger.error(f\"Error during memory test: {e}\")\n</code></pre> <p>To run memory tests with different configurations, use:</p> <pre><code>TEST_MEMORY_FRACTIONS=0.9,0.7,0.5 python -m tests.test_memory_manager\n</code></pre> <p>This document covers the key aspects of memory management in SAM Annotator. For further details, refer to the code documentation and comments in the relevant files: - <code>memory_manager.py</code>: Core memory management functionality - <code>image_utils.py</code>: Image processing cache implementation - <code>predictor.py</code>: Prediction caching systems </p>"},{"location":"placeholder/","title":"Coming Soon","text":"<p>This documentation section is currently under development. Please check back later.</p> <p>Return to Documentation Home </p>"},{"location":"shortcuts/","title":"SAM Annotator Keyboard Shortcuts","text":"<p>This document provides a comprehensive guide to the keyboard shortcuts available in the SAM Annotator tool. These shortcuts help users navigate the interface and perform operations efficiently.</p>"},{"location":"shortcuts/#basic-navigation","title":"Basic Navigation","text":"Action Shortcut Description Quit Q Exit the application Next Image N Navigate to the next image in the dataset Previous Image P Navigate to the previous image in the dataset Save S Save current annotations Clear Selection X Clear the current selection Add Annotation A Add the current selection as an annotation Undo Z Undo the last action Redo Y Redo the previously undone action Clear All C Clear all annotations on the current image"},{"location":"shortcuts/#view-controls","title":"View Controls","text":"Action Shortcut Description Toggle Masks M Show/hide segmentation masks Toggle Boxes B Show/hide bounding boxes Toggle Labels L Show/hide annotation labels Toggle Points T Show/hide prompt points Toggle View Controls V Show/hide view control panel Toggle Review Mode R Enter/exit annotation review mode"},{"location":"shortcuts/#export-operations","title":"Export Operations","text":"<p>SAM Annotator supports exporting annotations to various formats using a two-key sequence:</p> <ol> <li>Press E to enter Export Mode</li> <li>Press the format key:</li> <li>C for COCO format</li> <li>Y for YOLO format</li> <li>P for Pascal VOC format</li> </ol>"},{"location":"shortcuts/#zoom-controls","title":"Zoom Controls","text":"Action Shortcut Description Zoom In = Increase zoom level Zoom Out - Decrease zoom level Reset Zoom 0 Reset zoom to 100%"},{"location":"shortcuts/#annotation-opacity-controls","title":"Annotation Opacity Controls","text":"Action Shortcut Description Increase Opacity ] Make annotations more opaque Decrease Opacity [ Make annotations more transparent"},{"location":"shortcuts/#function-keys","title":"Function Keys","text":"Action Shortcut Description Help F1 Show help information Save View Settings F2 Save current view configuration Load View Settings F3 Load saved view configuration"},{"location":"shortcuts/#using-shortcuts-effectively","title":"Using Shortcuts Effectively","text":""},{"location":"shortcuts/#tips-for-efficient-annotation","title":"Tips for Efficient Annotation","text":"<ol> <li> <p>Learn the basics first: Focus on mastering A (add annotation), X (clear selection), and S (save) for the most common workflow.</p> </li> <li> <p>Navigation efficiency: Use N and P to quickly move through images without using the mouse.</p> </li> <li> <p>Customize view: Toggle various view elements (M, B, L) based on your current task to reduce visual clutter.</p> </li> <li> <p>Review workflow: Press R to enter review mode when you need to check your annotations without making accidental changes.</p> </li> </ol>"},{"location":"shortcuts/#example-workflows","title":"Example Workflows","text":""},{"location":"shortcuts/#basic-annotation-workflow","title":"Basic Annotation Workflow","text":"<ol> <li>Navigate to an image (N/P)</li> <li>Create a selection (using mouse)</li> <li>Add the annotation (A)</li> <li>Repeat steps 2-3 for all objects</li> <li>Save work (S)</li> <li>Move to next image (N)</li> </ol>"},{"location":"shortcuts/#review-and-correction-workflow","title":"Review and Correction Workflow","text":"<ol> <li>Enter review mode (R)</li> <li>Navigate through images (N/P)</li> <li>Exit review mode to make corrections (R again)</li> <li>Use undo/redo (Z/Y) to fix errors</li> <li>Save changes (S)</li> </ol>"},{"location":"shortcuts/#customizing-shortcuts","title":"Customizing Shortcuts","text":"<p>Shortcuts can be customized by modifying the <code>shortcuts.py</code> file in the configuration directory. The default location is:</p> <pre><code>sam_annotator/config/shortcuts.py\n</code></pre> <p>To customize shortcuts, edit the <code>SHORTCUTS</code> and <code>FUNCTION_SHORTCUTS</code> dictionaries in this file.</p> <p>Example of customization: <pre><code># Changing 'next_image' from 'n' to 'right arrow'\nSHORTCUTS = {\n    # ... other shortcuts ...\n    'next_image': 'Right',  # Changed from 'n'\n    # ... other shortcuts ...\n}\n</code></pre></p> <p>Note: After customizing shortcuts, restart the application for changes to take effect. </p>"}]}